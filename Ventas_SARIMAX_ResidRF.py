{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18abe8bb-e69c-4b0f-91db-7d24c3f5cc09",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'try' statement on line 14 (2673548466.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 16\u001b[1;36m\u001b[0m\n\u001b[1;33m    file_path = 'Datos.xlsx'\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'try' statement on line 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import openpyxl\n",
    "\n",
    "# Ruta del archivo\n",
    "file_path = 'Datos.xlsx'\n",
    "\n",
    "# Cargar el archivo de Excel\n",
    "data = pd.ExcelFile(file_path)\n",
    "\n",
    "# Inspeccionar los nombres de las hojas disponibles\n",
    "print(data.sheet_names)\n",
    "\n",
    "# Cargar la hoja relevante ('ventas')\n",
    "ventas_data = data.parse('ventas')\n",
    "\n",
    "# Inspeccionar las primeras filas del conjunto de datos\n",
    "print(ventas_data.head())\n",
    "\n",
    "# Convertir la columna 'fecha' a formato datetime para facilitar el manejo de fechas\n",
    "ventas_data['fecha'] = pd.to_datetime(ventas_data['fecha'])\n",
    "\n",
    "# Identificar la última fecha con un dato no nulo en la columna 'ventas'\n",
    "ultima_fecha_ventas = ventas_data.loc[ventas_data['ventas'].notna(), 'fecha'].max()\n",
    "print(f'Ultima fecha con datos de ventas: {ultima_fecha_ventas}')\n",
    "\n",
    "# Filtrar los datos hasta la última fecha con ventas disponibles\n",
    "ventas_hasta_actual = ventas_data[ventas_data['fecha'] <= ultima_fecha_ventas].copy()\n",
    "\n",
    "# Asegurar que las predicciones se extiendan hasta diciembre 2029\n",
    "fecha_fin_prediccion = '2029-12-01'\n",
    "fechas_futuras = pd.date_range(start=ultima_fecha_ventas + pd.Timedelta(days=1), end=fecha_fin_prediccion, freq='MS')\n",
    "\n",
    "# Crear DataFrame de predicción con las fechas futuras\n",
    "datos_para_predecir = pd.DataFrame({'fecha': fechas_futuras})\n",
    "\n",
    "# Agregar columnas explicativas vacías para las fechas futuras\n",
    "datos_para_predecir = datos_para_predecir.merge(ventas_data.drop(columns=['ventas']), on='fecha', how='left')\n",
    "datos_para_predecir['ventas'] = np.nan\n",
    "\n",
    "# Inicializar residuales y error objetivo\n",
    "error_objetivo =  1.95\n",
    "error_actual = float('inf')\n",
    "iteracion = 0\n",
    "\n",
    "# Ciclo de reentrenamiento hasta minimizar el error\n",
    "while error_actual > error_objetivo:\n",
    "    iteracion += 1\n",
    "    print(f\"Iteración {iteracion}:\")\n",
    "\n",
    "    # Ajustar SARIMAX automáticamente según ACF y PACF\n",
    "    def ajustar_sarimax(datos, orden):\n",
    "        modelo = SARIMAX(datos, order=orden, seasonal_order=(1, 0, 1, 12))\n",
    "        resultado = modelo.fit(disp=False)\n",
    "        return resultado\n",
    "\n",
    "    # Calcular ACF y PACF\n",
    "    ventas_series = ventas_hasta_actual['ventas']\n",
    "    orden_sarimax = (1, 1, 1)  # Basado en ACF y PACF\n",
    "\n",
    "    # Ajustar modelo SARIMAX\n",
    "    modelo_sarimax = ajustar_sarimax(ventas_series, orden_sarimax)\n",
    "    pronostico_sarimax = modelo_sarimax.forecast(steps=len(datos_para_predecir))\n",
    "    datos_para_predecir['pronostico_sarimax'] = pronostico_sarimax.values\n",
    "\n",
    "    # Preparar los datos para el modelo Random Forest\n",
    "    variables_explicativas = [\"igae\", \"msr\", \"empleo\", \"inpc\", \"cetes28\", \"easter\"]\n",
    "    X = ventas_hasta_actual[variables_explicativas].copy()\n",
    "    y = ventas_hasta_actual[\"ventas\"].copy()\n",
    "\n",
    "    # Crear características adicionales como rezagos\n",
    "    for lag in range(1, 4):\n",
    "        X[f'ventas_lag_{lag}'] = ventas_hasta_actual['ventas'].shift(lag)\n",
    "        datos_para_predecir[f'ventas_lag_{lag}'] = ventas_hasta_actual['ventas'].shift(lag).iloc[-len(datos_para_predecir):].values\n",
    "\n",
    "    # Limpiar los valores NaN generados por los rezagos en los datos históricos\n",
    "    X = X.dropna()\n",
    "    y = y.iloc[X.index]\n",
    "\n",
    "    # Escalar las variables explicativas\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Dividir los datos usando TimeSeriesSplit para respetar la secuencia temporal\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    param_grid = {\n",
    "        'n_estimators': [200, 300, 500],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_scaled, y)\n",
    "\n",
    "    # Mejor modelo\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "\n",
    "    # Predecir para las fechas futuras\n",
    "    required_columns = variables_explicativas + [f'ventas_lag_{lag}' for lag in range(1, 4)]\n",
    "    datos_para_predecir = datos_para_predecir.dropna(subset=required_columns)\n",
    "    if not datos_para_predecir.empty:\n",
    "        X_futuro = datos_para_predecir[required_columns]\n",
    "        X_futuro_scaled = scaler.transform(X_futuro)\n",
    "        predicciones_rf_futuro = best_rf.predict(X_futuro_scaled)\n",
    "\n",
    "        # Integrar los pronósticos de ambos modelos\n",
    "        datos_para_predecir['pronostico_rf'] = predicciones_rf_futuro\n",
    "\n",
    "        # Combinar pronósticos (promedio de ambos modelos)\n",
    "        datos_para_predecir['pronostico_combinado'] = (datos_para_predecir['pronostico_sarimax'] + datos_para_predecir['pronostico_rf']) / 2\n",
    "\n",
    "        # Calcular residuales y error\n",
    "        residuales = y - best_rf.predict(X_scaled)\n",
    "        mae = mean_absolute_error(y, best_rf.predict(X_scaled)).round(4)\n",
    "        error_actual = mae.round(2)\n",
    "        print(f\"Error MAE actual: {mae}\")\n",
    "\n",
    "        # Actualizar datos históricos con residuales ajustados\n",
    "        ventas_hasta_actual['ventas'] += residuales.mean()\n",
    "    else:\n",
    "        print(\"No hay suficientes datos futuros para realizar predicciones.\")\n",
    "        break\n",
    "\n",
    "# Exportar el pronóstico combinado a Excel\n",
    "output_file = 'pronostico_ventas_reentrenado.xlsx'\n",
    "datos_para_predecir.to_excel(output_file, index=False)\n",
    "print(f\"Pronóstico exportado a {output_file}\")\n",
    "\n",
    "# Calcular la importancia de las variables explicativas desde el modelo Random Forest ajustado\n",
    "importancia_variables = best_rf.feature_importances_\n",
    "importancia_porcentaje = (importancia_variables / importancia_variables.sum()) * 100\n",
    "\n",
    "# Crear un DataFrame para presentar los resultados\n",
    "importancia_df = pd.DataFrame({\n",
    "    'Variable': X.columns,  # Usar los nombres de las columnas originales del conjunto X\n",
    "    'Importancia (%)': importancia_porcentaje\n",
    "}).sort_values(by='Importancia (%)', ascending=False)\n",
    "\n",
    "# Imprimir la importancia de las variables\n",
    "print(\"Porcentaje de rendimiento de cada variable explicativa:\")\n",
    "print(importancia_df)\n",
    "\n",
    "# Crear un gráfico de pastel para visualizar la importancia de las variables\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(importancia_df['Importancia (%)'], \n",
    "        labels=importancia_df['Variable'], \n",
    "        autopct='%1.1f%%', \n",
    "        startangle=140)\n",
    "plt.title('Importancia de las Variables (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18d6723-a5c5-4811-942f-69f6d5f79691",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
